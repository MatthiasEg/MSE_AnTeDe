{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EflF7KkikDHO"
      },
      "source": [
        "![MSE Logo](https://moodle.msengineering.ch/pluginfile.php/1/core_admin/logocompact/300x300/1613732714/logo-mse.png \"MSE Logo\") \n",
        "\n",
        "# AnTeDe Lab 5: Latent Semantic Analysis with Gensim\n",
        "\n",
        "## Objective\n",
        "The goal of this lab is to perform LSA on a small corpus of news.  You will use the LSA word vectors to estimate word similarity, and then to perform ranked retrieval given a query. \n",
        "\n",
        "<font color='green'>Please answer the questions in green within this notebook, and submit the completed notebook under the corresponding homework on Moodle.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOLd-kq1kDHT",
        "outputId": "f47b1f38-d066-436c-c6ef-203d8e3c8ac8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: contractions in /usr/local/lib/python3.7/dist-packages (0.1.68)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.7/dist-packages (from contractions) (0.0.21)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (1.4.4)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (0.3.0)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "source": [
        "! pip install contractions\n",
        "import os    \n",
        "import nltk\n",
        "import gensim\n",
        "import pandas as pd\n",
        "from TextPreprocessor import *\n",
        "from gensim import models, corpora, similarities\n",
        "from gensim.models import LsiModel, LdaModel, LdaMulticore\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5miKh71kDHV"
      },
      "source": [
        "The data used in this lab the same set of 300 Australian that you used in Lab 4 on Document Representation.  It is a shortened version of the Lee Background Corpus [described here](http://www.socsci.uci.edu/~mdlee/lee_pincombe_welsh_document.PDF) and it is available with the **gensim** package that you installed.  The following code will load the documents into a Pandas dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "P0JCa6QXkDHW"
      },
      "outputs": [],
      "source": [
        "# Code inspired from:\n",
        "# https://github.com/bhargavvader/personal/blob/master/notebooks/text_analysis_tutorial/topic_modelling.ipynb\n",
        "\n",
        "test_data_dir = '{}'.format(os.sep).join([gensim.__path__[0], 'test', 'test_data'])\n",
        "lee_train_file = test_data_dir + os.sep + 'lee_background.cor'\n",
        "text = open(lee_train_file).read().splitlines()\n",
        "data_df = pd.DataFrame({'text': text})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9u7EywjkDHX"
      },
      "source": [
        "## Data preprocessing\n",
        "\n",
        "You will need first to preprocess the data through the following stages:\n",
        "1. tokenization\n",
        "2. stopword removal\n",
        "2. POS-based filtering\n",
        "3. lemmatization or stemming\n",
        "4. addition of bigrams to each document\n",
        "5. filtering of infrequent words\n",
        "6. inspection and filtering of frequent words\n",
        "\n",
        "You can use NLTK or our in-house `TextPreprocessor.py` file, as explained in Lab 1.\n",
        "\n",
        "<font color='green'>Please state here which solution you use and apply (a) POS-based filtering for adjectives and nouns only, (b) lemmatization not stemming. Addition of bigrams is not to be implemented for this lab. </font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "pIpQ7SXVkDHY"
      },
      "outputs": [],
      "source": [
        "# I'm using the textprocessor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "V_jqVbVWkDHY"
      },
      "outputs": [],
      "source": [
        "from nltk.tag import pos_tag\n",
        "# Please write here the preprocessing instructions if you use TextPreprocessor.py\n",
        "processor = TextPreprocessor(language='english', stopwords=set(stopwords.words('english')), pos_tags=set(['a', 'n']), lemmatize=True, stem=False, n_jobs=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "h--_tDSekDHZ"
      },
      "outputs": [],
      "source": [
        "data_df['processed'] = processor.transform(data_df['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "DHhFGr2DkDHa"
      },
      "outputs": [],
      "source": [
        "data_df['tokenized'] = data_df['processed'].apply(nltk.word_tokenize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "g2FCKNuvkDHc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a18e1cf2-4df5-40a8-c3ae-6c5e080a354f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['union', 'qantas', 'maintenance', 'worker', 'industrial', 'action', 'company', 'reject', 'offer', 'dispute', 'party', 'private', 'talk', 'yesterday', 'industrial', 'relation', 'commission', '3,000', 'maintenance', 'worker', 'reject', 'qantas', 'wage', 'freeze', 'national', 'secretary', 'australian', 'manufacturing', 'worker', 'union', 'amwu', 'doug', 'cameron', 'union', 'everything', 'possible', 'resolve', 'dispute', '``', 'qantas', 'prepared', 'accept', 'private', 'arbitration', 'alternative', 'worker', 'industrial', 'action', 'escalate', 'industrial', 'action', 'necessary', 'fair', 'company', 'crush', 'underfoot', \"''\"]\n"
          ]
        }
      ],
      "source": [
        "print(data_df['tokenized'].iloc[120])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xGkeVeukDHc"
      },
      "source": [
        "Please make a list of all words from all articles.  Then, using `nltk.FreqDist`, consider the most frequent and the least frequent words.  If you find uninformative words among the most frequent ones, please remove them from the articles.  Similarly, please remove from articles the words appearing fewer than 2 or 3 times in the corpus.  <font color='green'> Please justify these choices. What is now the size of your vocabulary?</font> "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "SV49G1CNkDHe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f71308a7-9d0d-4beb-d914-1405a4c0e5f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most frequent words (n=50): \n",
            " [('mr', 306), ('australian', 178), ('new', 171), ('palestinian', 168), ('australia', 157), ('people', 153), ('government', 146), ('two', 136), ('u', 136), ('day', 131), ('south', 130), ('state', 128), ('attack', 127), ('year', 126), ('would', 115), ('one', 114), ('israeli', 112), ('force', 109), ('minister', 106), ('last', 99), ('arafat', 96), ('fire', 95), ('afghanistan', 90), ('united', 89), ('three', 87), ('police', 85), ('world', 84), ('security', 83), ('official', 83), ('could', 82), ('time', 80), ('area', 79), ('today', 77), ('leader', 77), ('told', 75), ('group', 74), ('company', 73), ('union', 71), ('authority', 69), ('laden', 69), ('bin', 68), ('report', 67), ('sydney', 64), ('month', 63), ('man', 63), ('president', 62), ('bank', 61), ('around', 60), ('four', 60), ('test', 58)]\n",
            "Least frequent words (n=50): \n",
            " [('onto', 1), ('vinyl', 1), ('resentment', 1), ('withdrew', 1), ('limelight', 1), ('plagiarism', 1), ('reformation', 1), ('beset', 1), ('throat', 1), ('intruder', 1), ('uk', 1), ('mysticism', 1), ('dawn', 1), ('tent', 1), ('marquee', 1), ('rob', 1), ('recrimination', 1), ('congressman', 1), ('excess', 1), ('precursor', 1), ('mouse', 1), ('tissue', 1), ('variety', 1), ('parkinson', 1), ('epidemic', 1), ('ukraine', 1), ('insecurity', 1), ('steep', 1), ('merger', 1), ('stone', 1), ('muscle', 1), ('grouping', 1), ('academic', 1), ('centenary', 1), ('recognises', 1), ('lecturer', 1), ('bedeharris', 1), ('monarchy', 1), ('codification', 1), ('opporunity', 1), ('cedric', 1), ('pioline', 1), ('fabrice', 1), ('santoro', 1), ('appraisal', 1), ('pro', 1), ('con', 1), ('overcame', 1), ('sebastien', 1), ('grosjean', 1)]\n",
            "Least frequent words (n=10): [('hornsby', 4), ('ganges', 4), ('hindu', 4), ('beatles', 4), ('saxeten', 4), ('liverpool', 4), ('electricity', 4), ('arthur', 4), ('rubber', 4), ('cow', 4)]\n",
            "\n",
            "Before filtering: 4687\n",
            "After filtering: 1525\n"
          ]
        }
      ],
      "source": [
        "# Please write here all the necessary instructions.  You may use several cells.\n",
        "\n",
        "all_words = [w for ws in data_df['tokenized'] for w in ws if w.isalpha()]\n",
        "\n",
        "frequency_distribution = nltk.FreqDist(all_words)\n",
        "\n",
        "print(f'Most frequent words (n=50): \\n {frequency_distribution.most_common(50)}')\n",
        "print(f'Least frequent words (n=50): \\n {frequency_distribution.most_common()[-50:]}') \n",
        "\n",
        "filtered = dict((word, freq) for word, freq in frequency_distribution.items() if freq > 3)\n",
        "fdist_filtered = nltk.FreqDist(filtered)\n",
        "print(f'Least frequent words (n=10): {fdist_filtered.most_common()[-10:]}') \n",
        "\n",
        "vocab = set(word for word, _ in frequency_distribution.items())\n",
        "vocab_filtered = set(word for word, _ in fdist_filtered.items())\n",
        "print(f'\\nBefore filtering: {len(vocab)}')\n",
        "print(f'After filtering: {len(vocab_filtered)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "3JKRtHRLkDHe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc6bbb93-a90c-45c0-e35b-9ef10689c016"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['afghan', 'security', 'force', 'arab', 'al', 'qaeda', 'fighter', 'seven', 'others', 'weapon', 'explosive', 'remain', 'hospital', 'southern', 'city', 'kandahar', 'spokesman', 'governor', 'gul', 'agha', 'man', 'left', 'ward', 'one', 'arab', 'custody', 'ward', 'mr', 'seven', 'weapon', 'grenade', 'explosive', 'explosive', 'surrender', 'weapon', 'mr', 'concerned', 'safety', 'arab', 'u', 'bombing', 'kandahar', 'airport', 'hospital', 'taliban', 'militia', 'month', 'taliban', 'weapon', 'grenade', 'explosive', 'arab', 'could', 'protect', 'blow', 'hospital', 'room', 'attempt', 'arrest']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      [hundred, people, home, southern, highland, ne...\n",
              "1      [indian, security, force, shot, dead, eight, m...\n",
              "2      [national, road, toll, year, holiday, period, ...\n",
              "3      [argentina, political, economic, crisis, inter...\n",
              "4      [six, midwife, hospital, south, sydney, inappr...\n",
              "                             ...                        \n",
              "295    [team, australian, israeli, scientist, success...\n",
              "296    [today, world, aid, day, late, figure, show, m...\n",
              "297    [federal, national, party, possible, stage, op...\n",
              "298    [university, canberra, proposal, republic, one...\n",
              "299    [australia, france, double, rubber, davis, cup...\n",
              "Name: filtered, Length: 300, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ],
      "source": [
        "# filter the data\n",
        "data_df['filtered'] = [[w for w in ws if w in vocab_filtered] for ws in data_df['tokenized']]\n",
        "print(data_df['filtered'].iloc[50])\n",
        "data_df['filtered']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohmcqR1JkDHf"
      },
      "source": [
        "## LSA with Gensim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11qfV17-kDHf"
      },
      "source": [
        "In this section, you will write the Gensim commands to compute a term-document matrix from the above documents, then transform it using SVD, and truncate the result.  To learn what the commands are, please follow the [Topics and Tranformations tutorial](https://radimrehurek.com/gensim/auto_examples/core/run_topics_and_transformations.html) from Gensim. \n",
        "\n",
        "<font color=\"green\">Please gather these commands into a function called `train_lsa`.  They should cover: dictionary creation, corpus mapping, computation of TF-IDF values, and creation of the LSA model.</font> "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "QGPLIx6UkDHg"
      },
      "outputs": [],
      "source": [
        "def train_lsa(filtered_texts, num_topics = 10):\n",
        "\n",
        "  dictionary = corpora.Dictionary(filtered_texts)\n",
        "\n",
        "  corpus = [dictionary.doc2bow(text) for text in filtered_texts]\n",
        "\n",
        "  tfidf = models.TfidfModel(corpus, normalize=True)\n",
        "  corpus_tfidf = tfidf[corpus]\n",
        "\n",
        "  lsa = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=num_topics)\n",
        "\n",
        "  return lsa, dictionary, corpus, corpus_tfidf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRvImH6kkDHg"
      },
      "source": [
        "<font color=\"green\">Please fix the `number_of_topics` to 10, on the lower side of the range mentioned in the course.  Then, execute the cell that performs `train_lsa`.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "nloeyz_6kDHg"
      },
      "outputs": [],
      "source": [
        "number_of_topics = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "ifxArcSbkDHh"
      },
      "outputs": [],
      "source": [
        "lsa_model, dictionary, corpus, corpus_tfidf = train_lsa(data_df['filtered'], 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAi-i4E_kDHh"
      },
      "source": [
        "<font color=\"green\">Please display several topics found by LSA using the Gensim `print_topics` function using `num_topics=4` and `num_words=10`.  Please explain in your own words the meaning of what is displayed.  How do you relate it with what was explained in the course on LSA?</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "MgB6i6AakDHh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a7293e3-2646-42d6-92cd-2671d624b11e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0,\n",
              "  '0.349*\"palestinian\" + 0.230*\"israeli\" + 0.204*\"arafat\" + 0.129*\"israel\" + 0.127*\"mr\" + 0.123*\"hamas\" + 0.113*\"attack\" + 0.107*\"gaza\" + 0.106*\"force\" + 0.105*\"afghanistan\"'),\n",
              " (1,\n",
              "  '-0.421*\"palestinian\" + -0.278*\"israeli\" + -0.246*\"arafat\" + -0.153*\"israel\" + -0.150*\"hamas\" + 0.135*\"afghanistan\" + -0.132*\"gaza\" + 0.110*\"bin\" + 0.109*\"laden\" + -0.104*\"sharon\"')]"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ],
      "source": [
        "lsa_model.print_topics(number_of_topics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wglyo8GgkDHi"
      },
      "source": [
        "<font color=\"green\">Please define a function that returns the cosine similarity between two words (testing first if they are in the vocabulary). Please exemplify its value on the two word pairs \"morning\" and \"night\" as well as \"morning\" and \"qantas\", and two additional word pairs of your choice and comment the values.</font>  You can get inspiration from this [Gensim Tutorial on Document Similarity](https://radimrehurek.com/gensim/auto_examples/core/run_similarity_queries.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "7TAzWfeTkDHj"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "u2qlxDemkDHj"
      },
      "outputs": [],
      "source": [
        "def wordsim(word1, word2, model, dictionary):\n",
        "  word1_bow = dictionary.doc2bow([word1])\n",
        "  word2_bow = dictionary.doc2bow([word2])\n",
        "\n",
        "  if len(word1_bow) == 0 or len(word2_bow) == 0:\n",
        "        raise Exception('Words not in dictionary!')\n",
        "\n",
        "  # convert to LSA space\n",
        "  word1_lsa = model[word1_bow]\n",
        "  word2_lsa = model[word2_bow]\n",
        "    \n",
        "  # compute the similarity\n",
        "  sim = cosine_similarity(word1_lsa, word2_lsa)\n",
        "\n",
        "  return sim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "20GzQDjFkDHk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd5fa235-8096-43b9-9139-60be2f335777"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.         -0.26790075]\n",
            " [-0.23886647  0.99954958]]\n",
            "[[ 1.         -0.13071564]\n",
            " [ 0.00122172  0.99125946]]\n"
          ]
        }
      ],
      "source": [
        "# print here the cosine similiarities of several pairs and comment the results.\n",
        "sim_high = wordsim('arafat', 'israeli', lsa_model, dictionary)\n",
        "sim_low = wordsim('hindu', 'gaza', lsa_model, dictionary)\n",
        "\n",
        "print(sim_high)\n",
        "print(sim_low)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpbRticfkDHk"
      },
      "source": [
        "<font color=\"green\">Please use the [Gensim Tutorial on Document Similarity](https://radimrehurek.com/gensim/auto_examples/core/run_similarity_queries.html) to write a function that prints a list of words sorted by decreasing LSA similarity with a given word and showing the score too.  You don't have to use the cosine_similarity function here.  Please choose a \"query\" word and ten other words, apply your function, and comment the results.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "PSppnBdOkDHk"
      },
      "outputs": [],
      "source": [
        "from gensim import similarities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "z31y-PtZkDHk"
      },
      "outputs": [],
      "source": [
        "def word_ranking(word0, word_list, model, dictionary):\n",
        "  word0_bow = dictionary.doc2bow([word0])\n",
        "  words_bow = [dictionary.doc2bow([w]) for w in word_list]\n",
        "    \n",
        "  word0_lsa = model[word0_bow]\n",
        "  words_lsa = model[words_bow]\n",
        "    \n",
        "  index = similarities.MatrixSimilarity(words_lsa)\n",
        "    \n",
        "  # perform a similarity query against the corpus\n",
        "  sims = index[word0_lsa]  \n",
        "  sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
        "\n",
        "  for i, (doc_position, doc_score) in enumerate(sims):\n",
        "    print('{0}: \"{1}\"\", score: {2:.5f}'.format(i, word_list[doc_position], doc_score))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "YI_0dplJkDHk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d9158a8-8151-4fca-fbbe-66a6d19aa300"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: \"hamas\"\", score: 0.99999\n",
            "1: \"arafat\"\", score: 0.99994\n",
            "2: \"palestinian\"\", score: 0.99994\n",
            "3: \"israel\"\", score: 0.99984\n",
            "4: \"hot\"\", score: 0.21829\n",
            "5: \"force\"\", score: 0.14463\n",
            "6: \"hindu\"\", score: 0.00942\n",
            "7: \"black\"\", score: 0.00000\n",
            "8: \"wind\"\", score: -0.24296\n",
            "9: \"water\"\", score: -0.25200\n"
          ]
        }
      ],
      "source": [
        "# call here the function on your choice of words\n",
        "word_ranking('gaza', ['hot', 'water', 'palestinian', 'israel', 'hindu', 'black', 'wind', 'arafat', 'hamas', 'force'], lsa_model, dictionary)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " #### Comments\n",
        "\n",
        "The words hamas and arafat have a very high score with gaza, because they often appear together. Wind and water are not very common as context and thus have very low scores."
      ],
      "metadata": {
        "id": "IA7oY3Q41Uo9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpZXS6iUkDHl"
      },
      "source": [
        "<font color=\"green\">Please select now a significantly larger number of topics, and train a new LSA model.  Perform the same `word_ranking` task as above and compare the new ranking with the previous one.  Which one seems better?</font>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lsa_model, dictionary, corpus, corpus_tfidf = train_lsa(data_df['filtered'], num_topics=500)\n",
        "word_ranking('gaza', ['hot', 'water', 'palestinian', 'israel', 'hindu', 'black', 'wind', 'arafat', 'hamas', 'force'], lsa_model, dictionary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LipAoPWr0zTq",
        "outputId": "0cf030f7-7c4d-472b-846d-0a6668e3f916"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: \"palestinian\"\", score: 0.14483\n",
            "1: \"arafat\"\", score: 0.10987\n",
            "2: \"hamas\"\", score: 0.04349\n",
            "3: \"force\"\", score: 0.03042\n",
            "4: \"hindu\"\", score: 0.00719\n",
            "5: \"black\"\", score: 0.00000\n",
            "6: \"wind\"\", score: -0.00702\n",
            "7: \"water\"\", score: -0.01631\n",
            "8: \"hot\"\", score: -0.05414\n",
            "9: \"israel\"\", score: -0.07023\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mp8ZfBXCkDHl"
      },
      "source": [
        "## End of Lab 5\n",
        "Please make sure all cells have been executed, save this completed notebook, compress it to a *zip* file, and upload it to [Moodle](https://moodle.msengineering.ch/course/view.php?id=1869)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cours",
      "language": "python",
      "name": "cours"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "source": [],
        "metadata": {
          "collapsed": false
        }
      }
    },
    "colab": {
      "name": "MSE_AnTeDe_Lab5_LSA_Gensim_student (1).ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}