{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MSE Logo](https://moodle.msengineering.ch/pluginfile.php/1/core_admin/logo/0x150/1643104191/logo-mse.png)\n",
    "\n",
    "# AnTeDe Lab 4: Search Engine with the Vector Space Model\n",
    "\n",
    "## Summary\n",
    "The aim of this lab is to build a simple document search engine based on TF-IDF document vectors. \n",
    "\n",
    "The lab is inspired by a notebook designed by [Kavita Ganesan](https://github.com/kavgan/nlp-in-practice/blob/master/tf-idf/Keyword%20Extraction%20with%20TF-IDF%20and%20SKlearn.ipynb).\n",
    "\n",
    "<font color='green'>Please answer the questions in green within this notebook, and submit the completed notebook under the corresponding homework on Moodle.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os    \n",
    "import nltk  # on Colab, you mind find it helpful to run nltk.download('popular') to install packages\n",
    "import gensim\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from gensim import models, corpora, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TextPreprocessor import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data used in this lab is a set of 300 documents selected from the Australian Broadcasting Corporation's news mail service. It consists of texts of headline stories from around the years 2000-2001.  This is a shortened version of the Lee Background Corpus [described here](http://www.socsci.uci.edu/~mdlee/lee_pincombe_welsh_document.PDF).  It is available as test data in the **gensim** package, so you do not need to download it separately.\n",
    "\n",
    "The following code will load the documents into a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code inspired from:\n",
    "# https://github.com/bhargavvader/personal/blob/master/notebooks/text_analysis_tutorial/topic_modelling.ipynb\n",
    "\n",
    "test_data_dir = '{}'.format(os.sep).join([gensim.__path__[0], 'test', 'test_data'])\n",
    "lee_train_file = test_data_dir + os.sep + 'lee_background.cor'\n",
    "text = open(lee_train_file).read().splitlines()\n",
    "data_df = pd.DataFrame({'text': text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<WordListCorpusReader in 'C:\\\\Users\\\\Matthias\\\\AppData\\\\Roaming\\\\nltk_data\\\\corpora\\\\stopwords'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will run our in-house Text Preprocessor provided in the `TextPreprocessor.py` file, and documented in the `MSE_AnTeDe_TextPreprocessingDemo.ipynb` notebook provided in Lab 1 (see Lab 1 archive on Moodle for both files).\n",
    "\n",
    "<font color='green'>Please enrich the following code according your needs (especially stopwords)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = 'english'\n",
    "stop_words = set(stopwords.words(language))\n",
    "# Extend the list here:\n",
    "\n",
    "\n",
    "processor = TextPreprocessor(\n",
    "# Add options here:\n",
    " language=language,\n",
    " stopwords=stop_words,\n",
    " replace_contractions=True,\n",
    " lemmatize=True,\n",
    " n_jobs=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['processed'] = processor.transform(data_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new report suggests cost age australian population exaggerated report issue australia institute say detailed examination population health data show age population create unsustainable burden shrink workforce far economic social burden found majority old people enjoy healthy independent life many make financial contribution family participate voluntary community activity paper challenge assumption old population see health cost rise unsustainable level say rise health cost cause mainly factor age growth medical technology rise consumer demand escalate price\n"
     ]
    }
   ],
   "source": [
    "print(data_df['processed'].iloc[136])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation of document vectors with [Scikit-learn](https://scikit-learn.org/stable)\n",
    "\n",
    "We will use the [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) class from scikit-learn to create a vocabulary and generate word counts or *Term Frequencies* (TF).\n",
    "    \n",
    "The result is a  matrix representation of the counts: each column represents a _word_ in the vocabulary and each row represents a document in our dataset: the cell values are the word counts of the word in the document. \n",
    "\n",
    "The matrix is very sparse, because all words not appearing in a document have 0 counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(max_features=3000) # keep only the 3000 most frequent words in the corpus\n",
    "word_count_vector = cv.fit_transform(data_df['processed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hundreds of people have been forced to vacate ...</td>\n",
       "      <td>hundred people force vacate home southern high...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Indian security forces have shot dead eight su...</td>\n",
       "      <td>indian security force shot dead eight suspect ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The national road toll for the Christmas-New Y...</td>\n",
       "      <td>national road toll christmas-new year holiday ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Argentina's political and economic crisis has ...</td>\n",
       "      <td>argentina 's political economic crisis deepen ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Six midwives have been suspended at Wollongong...</td>\n",
       "      <td>six midwife suspend wollongong hospital south ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Hundreds of people have been forced to vacate ...   \n",
       "1  Indian security forces have shot dead eight su...   \n",
       "2  The national road toll for the Christmas-New Y...   \n",
       "3  Argentina's political and economic crisis has ...   \n",
       "4  Six midwives have been suspended at Wollongong...   \n",
       "\n",
       "                                           processed  \n",
       "0  hundred people force vacate home southern high...  \n",
       "1  indian security force shot dead eight suspect ...  \n",
       "2  national road toll christmas-new year holiday ...  \n",
       "3  argentina 's political economic crisis deepen ...  \n",
       "4  six midwife suspend wollongong hospital south ...  "
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some words from our vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Matthias\\.virtualenvs\\AnTeDe-FfWpwE78\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "feature_names = cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n",
      "['stance', 'stand', 'standard', 'star', 'start']\n",
      "1264\n"
     ]
    }
   ],
   "source": [
    "print(len(feature_names)) # has the max_features value been reached?\n",
    "print(feature_names[2500:2505]) # try various slices\n",
    "print(feature_names.index('hundred')) # find a word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TfidfTransformer to Compute Inverse Document Frequency (IDF)**\n",
    "\n",
    "We now use the (sparse) matrix generated by `CountVectorizer` to compute the IDF values of each word.  Note that the IDF should in reality be based on a large and representative corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer()"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer(smooth_idf=True, use_idf=True)\n",
    "tfidf_transformer.fit(word_count_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The IDF values are stored in the `idf_` field of the `TfidfTransformer`.  It has the same size as the array of feature names (words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n",
      "3.711377991194885\n"
     ]
    }
   ],
   "source": [
    "print(len(tfidf_transformer.idf_)) # check length\n",
    "print(tfidf_transformer.idf_[cv.get_feature_names().index('hundred')]) # check IDF value of a word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We define here two helper functions:**\n",
    " * the first one is a sorting function for the columns of a sparse matrix in COOrdinate format (a.k.a \"ijv\" or \"triplet\" format [explained here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.coo_matrix.html));\n",
    " * the second one extracts the feature names (*words*) and their TF-IDF values from the sorted list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "\n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    \"\"\"get the feature names and tf-idf score of top n items from sorted list\"\"\"\n",
    "    \n",
    "    #use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    "\n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "\n",
    "    for idx, score in sorted_items:\n",
    "        fname = feature_names[idx]\n",
    "        \n",
    "        #keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    "\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now select a document for which we will generate TF-IDF values.  <font color=\"green\">Please select a random document of your choice between 0 and 300.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_orig = data_df['text'].iloc[136]\n",
    "doc_processed = data_df['processed'].iloc[136]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The next instruction generates the vector of TF-IDF values for the document** using the `tfidf_transformer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vector = tfidf_transformer.transform(cv.transform([doc_processed]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we sort the words in the `tf_idf_vector` by decreasing TF-IDF values, first transforming the vector into a coordinate format ('coo'), and then applying our sorting function from above.  We then extract the words with the top 10 scores (and the scores) for the selected document using our second helper function from above and display them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A new report suggests the costs of an aging Australian population have been exaggerated. The report issued by the Australia Institute says a detailed examination of population and health data shows an aging population will not create an unsustainable burden on a shrinking workforce. Far from being an economic and social burden, it found the majority of older people enjoyed healthy and independent lives, many making financial contributions to their families and participating in voluntary community activities. The paper challenges the assumption an older population will see health costs rise to unsustainable levels. It says rising health costs are caused mainly by factors other than aging such as the growth of medical technology, rising consumer demand and escalating prices.  \n",
      " {'population': 0.407, 'age': 0.289, 'cost': 0.276, 'rise': 0.265, 'health': 0.265, 'unsustainable': 0.257, 'burden': 0.24, 'old': 0.122, 'voluntary': 0.12, 'participate': 0.12}\n"
     ]
    }
   ],
   "source": [
    "sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "\n",
    "topn_words = extract_topn_from_vector(feature_names, sorted_items, 10)\n",
    "\n",
    "print(doc_orig, '\\n', topn_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\">Please comment briefly on the relevance of these words with respect to the document content.</font>\n",
    "\n",
    "These words appear to have the highest relevance for the given document according to the TF-IDF score. Meaning, they appear often in this document, but are rather rare in all other documents of the corpus.\n",
    "This helps to adjust for the fact that some words appear more frequenctly in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document-document similarity using scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will write the commands to compute a document-document similarity matrix over the above documents, in scikit-learn.\n",
    "\n",
    "Please use a processing [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline) and a [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) and compute the *cosine similarities* between all documents.  \n",
    "\n",
    "<font color=\"green\">At the end, you will be asked to display the five most similar documents to the one you selected above, and compare the 1st and the 5th best results.</font>\n",
    "\n",
    "You can use inspiration from: \n",
    " * the above code\n",
    " * https://kavita-ganesan.com/tfidftransformer-tfidfvectorizer-usage-differences/#.XkK2ceFCe-Y\n",
    " * https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html\n",
    " * https://stackoverflow.com/questions/12118720/python-tf-idf-cosine-to-find-document-similarity\n",
    " * https://markhneedham.com/blog/2016/07/27/scitkit-learn-tfidf-and-cosine-similarity-for-computer-science-papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(use_idf=True)\n",
    "pipe = Pipeline(steps=[('pre', processor), ('tfidf', tfidf)]) # the 'processor' was defined above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>Please write a function called `find_similar` which receives a `tfidf_matrix` with all similarity scores between documents, and the `index` of a document in the collection, and returns the `top_n` most similar documents to it using cosine similarity.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar(tfidf_matrix, index, top_n = 5):\n",
    "  \n",
    "  cosine_similarities = linear_kernel(tfidf_matrix[index:index+1], tfidf_matrix).flatten()\n",
    "  related_docs_indices = [i for i in cosine_similarities.argsort()[::-1] if i != index]\n",
    "  return [(index, cosine_similarities[index]) for index in related_docs_indices][0:top_n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\">Using the data from the Pandas form created above, please use \"fit\" and \"transform\" to generate the matrix of all document similarites called \"tfidf_matrix\". -- How long do these two operations take on your computer?  -- Please explain briefly in your own words what is the difference between \"fit\" and \"transform\".</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 49.6436 secs\n"
     ]
    }
   ],
   "source": [
    "from time import perf_counter\n",
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def catchtime() -> float:\n",
    "    start = perf_counter()\n",
    "    yield lambda: perf_counter() - start\n",
    "\n",
    "\n",
    "with catchtime() as t:\n",
    "  tfidf_matrix = pipe.fit_transform(data_df['text'])\n",
    "\n",
    "print(f\"Execution time: {t():.4f} secs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training time\n",
    "It took 23.0933  seconds to proprocess the data and TFIDF vectorize it.\n",
    "\n",
    "#### Difference between fit and transform\n",
    "In general, the fit function only analyzes the data according to the given algorithm but does not change it. The change is only applied when transform function is invoked. Even then it is not pass by reference, but pass by value.\n",
    "Meaning one must assign the return value to the variable in order to really change it or can make used of \"in_place\" parameter which is often available.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\">Using `find_similar` and the `tfidf_matrix` please display the five most similar documents to the one you selected above, with their scores, comment them, and compare the 1st and the 5th best results.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_similar_documents = find_similar(tfidf_matrix, 136, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(123, 0.1826344185911181),\n",
       " (296, 0.14513522792781067),\n",
       " (193, 0.12311380670063467),\n",
       " (172, 0.11812106631098689),\n",
       " (209, 0.11636633800929484)]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_5_similar_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target document:\n",
      " A new report suggests the costs of an aging Australian population have been exaggerated. The report issued by the Australia Institute says a detailed examination of population and health data shows an aging population will not create an unsustainable burden on a shrinking workforce. Far from being an economic and social burden, it found the majority of older people enjoyed healthy and independent lives, many making financial contributions to their families and participating in voluntary community activities. The paper challenges the assumption an older population will see health costs rise to unsustainable levels. It says rising health costs are caused mainly by factors other than aging such as the growth of medical technology, rising consumer demand and escalating prices. \n",
      "Most similar document:\n",
      " A new report has revealed there are fewer young people using homeless services than widely thought. The Australian Institute of Health and Welfare report shows just over 1 per cent of people aged between 15 and 24 used such services over the past year. The main reason for young people seeking assistance was based on family or relationship difficulties. The report suggests the older people become, the less they use homeless services. \n"
     ]
    }
   ],
   "source": [
    "print(f\"Target document:\\n {data_df['text'].iloc[136]}\")\n",
    "print(f\"Most similar document:\\n {data_df['text'].iloc[top_5_similar_documents[0][0]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that both documents start very similar. Also, both are about reports of the Australian Institute which involve the population of Australia and there behavior. The word family does also appear in both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target document:\n",
      " A new report suggests the costs of an aging Australian population have been exaggerated. The report issued by the Australia Institute says a detailed examination of population and health data shows an aging population will not create an unsustainable burden on a shrinking workforce. Far from being an economic and social burden, it found the majority of older people enjoyed healthy and independent lives, many making financial contributions to their families and participating in voluntary community activities. The paper challenges the assumption an older population will see health costs rise to unsustainable levels. It says rising health costs are caused mainly by factors other than aging such as the growth of medical technology, rising consumer demand and escalating prices. \n",
      "5th most similar document:\n",
      " The Federal Government has confirmed there is a blowout in the Defence budget because of the cost of sending troops to Afghanistan. Defence Minister Robert Hill says the Government will have to consider delaying some Defence projects because of the cost of sending troops to the war against terrorism, but says no projects will be scrapped. Senator Hill says the cost of the deployment means some projects outlined in last year's Defence white paper will have to be reprioritised. He says the Government's options are to boost defence spending or to delay some projects. \"Now what I'm saying is you can't enter into major undertakings like contributing to the war against terrorism without knowing it's going to cost extra money,\" he said. \"We have to fund that, there are a range of options to do that.\" The Federal Opposition says Senator Hill must reveal which projects would be affected. Shadow Defence Minister Chris Evans says the Government should also come clean on the cost of using the Navy to intercept asylum seekers. \n"
     ]
    }
   ],
   "source": [
    "print(f\"Target document:\\n {data_df['text'].iloc[136]}\")\n",
    "# print([f\"{i+1}th document:\\n {data_df['text'].iloc[document_index]}\" for i, (document_index, _) in enumerate(top_5_similar_documents)])\n",
    "print(f\"5th most similar document:\\n {data_df['text'].iloc[top_5_similar_documents[4][0]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both texts are about certain costs and about how a certain institution or goverment can adjust the spending. Compared to the most similar one, we can see that these two texts definitly are less similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>Could you also use the dot product instead of the cosine similarity in the `find_similar` function?  Please answer in the following box.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes we could implement the cosine similarity by ourselfs, which actually is defined by the dot product. More specificly, the dot product of two vectors devided by the product of the length of the two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a search engine using Gensim\n",
    "\n",
    "<font color='green'>Using the [tutorial on Topics and Transformations from Gensim](https://radimrehurek.com/gensim/auto_examples/core/run_topics_and_transformations.html#sphx-glr-auto-examples-core-run-topics-and-transformations-py), please implement a method that returns the documents most similar to a given query.\n",
    "    \n",
    "Use [Gensim's TF-IDF Model](https://radimrehurek.com/gensim/models/tfidfmodel.html) to build the model and the [MatrixSimilarity function](https://radimrehurek.com/gensim/similarities/docsim.html#gensim.similarities.docsim.MatrixSimilarity) to measure cosine similarity between documents.</font>\n",
    "\n",
    "<font color='green'>Please write a query of your own (5-10 words), retrieve the 5 most similar documents, and comment the result.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import TfidfModel\n",
    "from gensim.similarities import MatrixSimilarity\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(documents: list):\n",
    "    # remove words that appear only once\n",
    "    frequency = defaultdict(int)\n",
    "    for text in documents:\n",
    "        for token in text:\n",
    "            frequency[token] += 1\n",
    "\n",
    "    documents = [\n",
    "        [token for token in text if frequency[token] > 1]\n",
    "        for text in documents\n",
    "    ]\n",
    "\n",
    "    dictionary = corpora.Dictionary(documents)\n",
    "    return [dictionary.doc2bow(text) for text in documents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = preprocess(data_df['processed'].to_list())\n",
    "gensim_tfidf_model = TfidfModel(corpus)\n",
    "corpus_tfidf = gensim_tfidf_model[corpus]\n",
    "\n",
    "five_most_similar_documents = MatrixSimilarity(corpus_tfidf, num_features=len(corpus_tfidf), num_best=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = preprocess(['Is there any sign of life in Australia or not?'])\n",
    "most_similar_to_query = five_most_similar_documents[query]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar document:\n",
      " Hundreds of people have been forced to vacate their homes in the Southern Highlands of New South Wales as strong winds today pushed a huge bushfire towards the town of Hill Top. A new blaze near Goulburn, south-west of Sydney, has forced the closure of the Hume Highway. At about 4:00pm AEDT, a marked deterioration in the weather as a storm cell moved east across the Blue Mountains forced authorities to make a decision to evacuate people from homes in outlying streets at Hill Top in the New South Wales southern highlands. An estimated 500 residents have left their homes for nearby Mittagong. The New South Wales Rural Fire Service says the weather conditions which caused the fire to burn in a finger formation have now eased and about 60 fire units in and around Hill Top are optimistic of defending all properties. As more than 100 blazes burn on New Year's Eve in New South Wales, fire crews have been called to new fire at Gunning, south of Goulburn. While few details are available at this stage, fire authorities says it has closed the Hume Highway in both directions. Meanwhile, a new fire in Sydney's west is no longer threatening properties in the Cranebrook area. Rain has fallen in some parts of the Illawarra, Sydney, the Hunter Valley and the north coast. But the Bureau of Meteorology's Claire Richards says the rain has done little to ease any of the hundred fires still burning across the state. \"The falls have been quite isolated in those areas and generally the falls have been less than about five millimetres,\" she said. \"In some places really not significant at all, less than a millimetre, so there hasn't been much relief as far as rain is concerned. \"In fact, they've probably hampered the efforts of the firefighters more because of the wind gusts that are associated with those thunderstorms.\" \n"
     ]
    }
   ],
   "source": [
    "print(f\"Most similar document:\\n {data_df['text'].iloc[most_similar_to_query[0][0][0]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2nd similar document:\n",
      " A new study shows that nearly one third of the Aboriginal and Torres Strait Islander population in Australia have been arrested in the past five years. The study conducted by the Australian National University for the New South Wales Bureau of Crime Statistics is the first to compare the arrest rates of the Aboriginal and non-Aboriginal population. It finds that unemployment, alcohol and assault rates were the main causes. Study author Boyd Hunter says policy both on a community and government level must deal with these issues if the arrest rate is to be decreased. \"Addressing the supply of alcohol in remote communities is seen as the most likely avenue for reducing rates of abuse, alcohol abuse and hence reduce arrest rates in those communities,\" he said. \n"
     ]
    }
   ],
   "source": [
    "print(f\"2nd similar document:\\n {data_df['text'].iloc[most_similar_to_query[0][1][0]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3nd similar document:\n",
      " President General Pervez Musharraf says Pakistan wants to defuse the brewing crisis with India, but was prepared to respond vigorously to any attack. \"Pakistan stands for peace, Pakistan wants peace, Pakistan wants to reduce tension,\" he said. \"Let the two countries move towards peace and harmony. \"However, Pakistan has taken all counter measures, if any war is thrust on Pakistan, the Pakistan armed forces and the 140 million people of Pakistan are fully prepared to face all consequences with all their might.\" The President said he had received the \"support of all political parties\". President Musharraf also said he welcomed the intervention of the international community in trying to defuse the potentially explosive crisis. \"We would like anybody to play a useful and positive role in defusing the tension.\" The United States, the European Union and the Group of eight industrialised nations among others, have all called on India and Pakistan to exercise restraint and resolve the stand-off through dialogue. President Musharraf repeated his offer of holding talks with Indian Prime Minister Atal Behari Vajpayee. \"I am for dialogue and I keep on saying this and India keeps on rejecting which gives me a feeling that I am begging to India. If they accept it we do not reject it at all,\" he said. On Friday he said he was willing to meet Prime Minister Vajpayee on the sidelines of the January 4-6 South Asian Association for Regional Cooperation (SAARC) summit in Nepal. India ruled out any face-to-face talks. Military tensions erupted between India and Pakistan after the bloody December 13 raid on the Indian parliament. India accuses Pakistan's military intelligence of masterminding the assault, but Pakistan denies the allegation. With both countries massing troops along the border, Pakistan Foreign Minister Abdul Sattar warned Saturday that the dispute was growing \"dangerously  tense\" and any small act of provocation could snowball into conflict. President Musharraf said one of the goals of Sunday's meeting was \"to take stock of the internal situation, the domestic environment\". \"I want to eradicate militancy, extremism, intolerance from Pakistani society and I also said I would like to eradicate any form of terrorism from the soil of Pakistan.\" However he warned the \"tension that has mounted on our eastern border in fact is creating obstacles and hurdles\". \n"
     ]
    }
   ],
   "source": [
    "print(f\"3nd similar document:\\n {data_df['text'].iloc[most_similar_to_query[0][2][0]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4th similar document:\n",
      " Malaysian police have arrested a man believed to have smuggled thousands of boat people into Australia. The arrest comes after a two-year investigation by the Australian Federal Police (AFP) and Department of Immigration. Naeil Ahmad Abdullah, 41, was arrested in Malaysia last month for allegedly transporting thousands of boat people from the Middle East to Indonesia and into Australia. AFP Commissioner Mick Keelty says the arrest will have a significant impact on people smuggling. \"What we often forget is this is transnational crime at its best,\" he said. The AFP says the arrest would not have happened without the coordinated effort of Malaysian and Australian authorities and believes it will lead to further arrests. \n"
     ]
    }
   ],
   "source": [
    "print(f\"4th similar document:\\n {data_df['text'].iloc[most_similar_to_query[0][3][0]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5th similar document:\n",
      " The United States says a video tape found inside Afghanistan proves beyond doubt Osama bin Laden was behind the attacks on the World Trade Centre and the Pentagon. The tape is alleged to show bin Laden discussing the success of the mission. In the 40-minute tape,  bin Laden is said to be at a dinner when told a plane had crashed into the World Trade Centre. He is alleged to have told others present what had happened and they cheered. US Vice-President Dick Cheney says the video shows bin Laden was clearly behind the attacks. \"There've some disputes in some quarters about it, but this is one more piece of evidence confirming his responsibility,\" he said. Republican Chuck Hagel of the Foreign Relations Committee says the administration must make the tapes public. \"The world needs to see this,\" he said. \"Some officials hope it will be shown to counter concerns in the Muslim world that bin Laden has been unjustly accused. Osama bin Laden was said to be staging a defiant stand in the Afghan mountains, as Taliban rule finally came to an ignominious end with the surrender of the last province under their control. A spokesman for the Northern Alliance said bin Laden was now leading the defence of his mountain hideouts in person, with about 1,000 loyal fighters from his Al Qaeda organisation. \"Osama himself has taken the command of the fighting,\" Mohammad Amin told the Reuters news agency from the eastern city of Jalalabad. \"He, along with around 1,000 of his people, including some Taliban officials, have now dug themselves into the forests of Spin Ghar after we overran all their bases in Tora Bora. \"He is here for sure,\" Mr Amin said. \"American planes have been carrying out regular and severe bombings to kill him.\" Mr Amin added that at least one of bin Laden's Arab fighters had been killed in \"very intense\" fighting. The Saudi-born Islamist accused by Washington of ordering the September 11 attacks on the United States appeared ever more isolated after his Taliban protectors handed over the Zabul province to tribal elders. \"The rule of the Taliban in Afghanistan has totally ended,\" the Pakistan-based Afghan news agency Afghan Islamic Press (AIP) said in reporting the surrender of Zabul. At least 24 civilians were killed and 15 injured in weekend bombing raids by US warplanes in Afghanistan's south-eastern Paktika province, the Afghan Islamic Press (AIP) said late Sunday. The Pakistan-based news agency, quoting informed sources, said the US jets blasted several vehicles at Sharana, the provincial capital of Paktika, on Saturday, killing 14 people and injuring several others. The dead were five children, four women and five men. Another 10 people were killed when US planes bombarded vehicles in pre-dawn raids on Sunday in the Mosh Khil area near Sharana, AIP said. It said a mosque was destroyed in the raids. AIP said Taliban rule had been ended in Paktika and the administration was being run by a tribal Shura (council). \n"
     ]
    }
   ],
   "source": [
    "print(f\"5th similar document:\\n {data_df['text'].iloc[most_similar_to_query[0][4][0]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like it somehow works. Australia appears in some of the texts, but not in all. What is interesting is that it seems to be able to match Sydney to Australia, which is nice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Lab 4\n",
    "Please make sure all cells have been executed, save this completed notebook, compress it to a *zip* file, and upload it to [Moodle](https://moodle.msengineering.ch/course/view.php?id=1869)."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "65cdad09e07dc93396c4dab55d3b849e26742f03ca3cf673993715944fdcc8b9"
  },
  "kernelspec": {
   "display_name": "cours",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
