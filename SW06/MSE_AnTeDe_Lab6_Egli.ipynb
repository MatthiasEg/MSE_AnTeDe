{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "solid-evening",
      "metadata": {
        "id": "solid-evening"
      },
      "source": [
        "![MSE Logo](https://moodle.msengineering.ch/pluginfile.php/1/core_admin/logo/0x150/1643104191/logo-mse.png)\n",
        "\n",
        "# AnTeDe Lab 6: Applications of word2vec\n",
        "## Objective\n",
        "* Compare pre-trained word2vec models with models trained on your workstation, on word similarity and analogy tasks.\t\n",
        "\n",
        "## General instructions\n",
        "* You can do the lab alone or in groups of two students.\n",
        "* Please write the required code, and also reply explicitly to the questions, as Python comments in code cells or text in markdown cells. \n",
        "* To submit your practical work, please make sure all cells are executed, then save and zip the notebook, and submit it as homework on [Moodle](https://moodle.msengineering.ch/course/view.php?id=1869).\n",
        "* Useful documentation: [section on word2vec in Gensim](https://radimrehurek.com/gensim/models/word2vec.html) as well as the [section on KeyedVectors in Gensim](https://radimrehurek.com/gensim/models/keyedvectors.html).\n",
        "* Training can be done locally if you have at least 16 GB of memory (it takes minutes, not hours), or using [Google Colab](https://colab.research.google.com/)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "incorrect-feedback",
      "metadata": {
        "id": "incorrect-feedback"
      },
      "source": [
        "## 1. Testing a word2vec model trained on Google News\n",
        "a. Install Gensim the latest version of Gensim, for instance by running in your Conda environment `pip install --upgrade gensim`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "concerned-richardson",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "concerned-richardson",
        "outputId": "21a8c151-b5fa-4cc2-f30c-665743304380"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: gensim\n",
            "Version: 3.6.0\n",
            "Summary: Python framework for fast Vector Space Modelling\n",
            "Home-page: http://radimrehurek.com/gensim\n",
            "Author: Radim Rehurek\n",
            "Author-email: me@radimrehurek.com\n",
            "License: LGPLv2.1\n",
            "Location: /usr/local/lib/python3.7/dist-packages\n",
            "Requires: six, smart-open, scipy, numpy\n",
            "Required-by: \n"
          ]
        }
      ],
      "source": [
        "# If you want to install from here, run the command ('!' indicates a command for the shell)\n",
        "#!pip install --upgrade gensim\n",
        "#!pip install nltk jupyter-notebook\n",
        "\n",
        "# Please run the following verification:\n",
        "!pip show gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "revolutionary-mobile",
      "metadata": {
        "id": "revolutionary-mobile"
      },
      "outputs": [],
      "source": [
        "import gensim, os\n",
        "from gensim import downloader\n",
        "# help(gensim.models.word2vec) # check if you are curious, but don't include output in the final notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "opposed-thirty",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "opposed-thirty",
        "outputId": "dc79a320-f869-4298-a9ed-e69cc4a4d032"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[==------------------------------------------------] 4.3% 72.2/1662.8MB downloaded"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-058f99a720d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Download the model file from Gensim, for the first time only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word2vec-google-news-300\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# We can assign the returned value to a model, but it is twice larger than needed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/downloader.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, return_path)\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m         \u001b[0m_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/downloader.py\u001b[0m in \u001b[0;36m_download\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"{fname}.gz\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0mdst_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m         \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreporthook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_progress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_calculate_md5_checksum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_get_checksum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    281\u001b[0m                 \u001b[0mblocknum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mreporthook\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m                     \u001b[0mreporthook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocknum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mread\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/downloader.py\u001b[0m in \u001b[0;36m_progress\u001b[0;34m(chunks_downloaded, chunk_size, total_size, part, total_parts)\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0mbar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpercent_downloaded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"%\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_downloaded\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1024\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m                 round(float(total_size) / (1024 * 1024), 1))\n\u001b[0m\u001b[1;32m     97\u001b[0m         )\n\u001b[1;32m     98\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, string)\u001b[0m\n\u001b[1;32m    406\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_schedule_flush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwritelines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36m_schedule_flush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_schedule_in_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_io_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_later\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush_interval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flush\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_schedule_in_thread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mschedule\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_events\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;31m# wake event thread (message content is ignored)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data, flags, copy, track, routing_id, group)\u001b[0m\n\u001b[1;32m    545\u001b[0m                 )\n\u001b[1;32m    546\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSocket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msend_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg_parts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._send_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Download the model file from Gensim, for the first time only\n",
        "gensim.downloader.load(\"word2vec-google-news-300\", return_path = True)\n",
        "# We can assign the returned value to a model, but it is twice larger than needed."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "structural-rescue",
      "metadata": {
        "id": "structural-rescue"
      },
      "source": [
        "b. Where is the model stored on your computer?  What is the size of the file?  Please store the absolute path in a variable called `path_to_model_file`, and use `os.path.getsize` to display the size converted in gigabytes with two decimals."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rural-burns",
      "metadata": {
        "id": "rural-burns"
      },
      "outputs": [],
      "source": [
        "# Please write your Python code below and execute it.\n",
        "path_to_model_file = '/root/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz'\n",
        "size_in_bytes = os.path.getsize(path_to_model_file)\n",
        "size_in_gb = size_in_bytes/(1024**3)\n",
        "print(f'{size_in_gb:.2f} GB')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "designed-cooling",
      "metadata": {
        "id": "designed-cooling"
      },
      "outputs": [],
      "source": [
        "# Load the model into the notebook:\n",
        "from gensim.models import KeyedVectors\n",
        "wv_model = gensim.models.KeyedVectors.load_word2vec_format(path_to_model_file, binary=True)  # C bin format"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "transsexual-mistress",
      "metadata": {
        "id": "transsexual-mistress"
      },
      "source": [
        "c. What is the memory size of the process corresponding to this notebook?  Please simply write the value you obtain from any OS-specific utility that you wish to use for this purpose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "recent-paper",
      "metadata": {
        "id": "recent-paper"
      },
      "outputs": [],
      "source": [
        "print('4.2 GB')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lightweight-constraint",
      "metadata": {
        "id": "lightweight-constraint"
      },
      "source": [
        "d. What is the size of the vocabulary of this model?  (I.e., how many words does it know?)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cleared-organizer",
      "metadata": {
        "id": "cleared-organizer"
      },
      "outputs": [],
      "source": [
        "# Please write the Python code needed to display the vocabulary size and execute it.\n",
        "print(len(wv_model.vectors))\n",
        "# or\n",
        "print(len(wv_model))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "stuck-wrong",
      "metadata": {
        "id": "stuck-wrong"
      },
      "source": [
        "e. Compare the vocabulary size with the number of words in an English dictionary.  How do you explain the difference?  Illustrate your explanation by showing at least 5 words which are in the model's vocabulary, and 2 that are not."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kdIn8_lq2UY9",
      "metadata": {
        "collapsed": false,
        "id": "kdIn8_lq2UY9"
      },
      "source": [
        "On [Wikipedia](https://en.wikipedia.org/wiki/List_of_dictionaries_by_number_of_words) the english language is stated to have around 470,000 Words.\n",
        "**Words that are available are:**\n",
        "- work\n",
        "- fun\n",
        "- school\n",
        "- computer\n",
        "- english\n",
        "\n",
        "**Words not in the dict are:**\n",
        "- covid - new word appeared 2020\n",
        "- woofits  - \"A hangover.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "forty-crossing",
      "metadata": {
        "id": "forty-crossing"
      },
      "outputs": [],
      "source": [
        "# Please write your Python code below and execute it.\n",
        "# words in dict\n",
        "print('work' in wv_model)\n",
        "print('fun' in wv_model)\n",
        "print('school' in wv_model)\n",
        "print('computer' in wv_model)\n",
        "print('english' in wv_model)\n",
        "\n",
        "\n",
        "# words not in dict\n",
        "print('covid' in wv_model)\n",
        "print('woofits' in wv_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aw58ev9o2UY_",
      "metadata": {
        "id": "aw58ev9o2UY_",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "print(wv_model['computer'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "celtic-energy",
      "metadata": {
        "id": "celtic-energy"
      },
      "source": [
        "f. Determine the size of the vector space for this word2vec model, i.e. the dimensionality of the embedding space, using two methods: either using the vector of a word from the vocabulary, or directly using the shape of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ahead-newport",
      "metadata": {
        "id": "ahead-newport"
      },
      "outputs": [],
      "source": [
        "# Please write your Python code below and execute it.\n",
        "print(len(wv_model['computer']))\n",
        "# or\n",
        "print(wv_model.vector_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "whole-roommate",
      "metadata": {
        "id": "whole-roommate"
      },
      "source": [
        "## 2. Using word2vec trained on Google News for word similarity\n",
        "In this section, you are going to use word vectors to compute (cosine) similarity between words (use the [KeyedVectors](https://radimrehurek.com/gensim/models/keyedvectors.html) documentation).  You will experiment with three tasks: (a) rank a small number of word pairs by decreasing similarity; (b) test the model on the WordSimilarity-353 set; (c) test the model on the analogy task."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "extraordinary-conspiracy",
      "metadata": {
        "id": "extraordinary-conspiracy"
      },
      "source": [
        "a. Sort the word pairs given below by decreasing similarity (i.e. most similar first).  Display also the similarity value found by word2vec, with 2 decimals."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "caring-relationship",
      "metadata": {
        "id": "caring-relationship"
      },
      "outputs": [],
      "source": [
        "test_pairs = [('car','automobile'), ('car', 'bike'), ('car', 'oil'), ('car', 'pedal'), \n",
        "              ('bike', 'pedal'), ('bike', 'bicycle'), ('oil', 'gas'), ('car', 'bus')]\n",
        "# Please write your Python code below and execute it.\n",
        "similarities = [(wv_model.similarity(tuple[0], tuple[1]), tuple) for tuple in test_pairs]\n",
        "similarities.sort(key=lambda x: x[0], reverse=True)\n",
        "print(similarities)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "impossible-incentive",
      "metadata": {
        "id": "impossible-incentive"
      },
      "source": [
        "b. What are the five closest words to *car* in the whole vocabulary and their similarity values with *car*? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "baking-shock",
      "metadata": {
        "id": "baking-shock"
      },
      "outputs": [],
      "source": [
        "wv_model.init_sims(replace=True) # run this to avoid memory footprint doubling with the first call\n",
        "# of \"most_similar\" (which caches unit vectors without replacement, unless told explicitly to do so).\n",
        "# Will have the same effect on evaluate_word_analogies below\n",
        "# Please write your Python code below and execute it.\n",
        "print(wv_model.most_similar('car', topn=5))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "requested-manner",
      "metadata": {
        "id": "requested-manner"
      },
      "source": [
        "c. Using the [KeyedVectors](https://radimrehurek.com/gensim/models/keyedvectors.html) documentation, evaluate the model on the WordSimilarity-353 task.  This compares similarities assigned to word pairs by word2vec with those assigned by humans.  Please display only the Pearson Correlation Coefficient, with two decimals."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "metallic-salmon",
      "metadata": {
        "id": "metallic-salmon"
      },
      "outputs": [],
      "source": [
        "# Please write your Python code below and execute it.\n",
        "from gensim.test.utils import datapath\n",
        "pearson, spearman, oov_ratio = wv_model.evaluate_word_pairs(datapath('wordsim353.tsv'))\n",
        "print(f'Pearson {pearson[0]:2f} with p-value {pearson[1]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "accessible-clone",
      "metadata": {
        "id": "accessible-clone"
      },
      "source": [
        "d. Using the [KeyedVectors](https://radimrehurek.com/gensim/models/keyedvectors.html) documentation, evaluate the model on the Analogy Tasks (e.g., \"*What is to Thailand what Athens is to Greece?*\"). The task is specified in a data file called `questions-words.txt`.  Note: this takes around 5 minutes.  Store the output in a variable for future use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "animated-faculty",
      "metadata": {
        "id": "animated-faculty"
      },
      "outputs": [],
      "source": [
        "# Please write your Python code below and execute it.\n",
        "analogy_scores = wv_model.evaluate_word_analogies(datapath('questions-words.txt'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "raising-harrison",
      "metadata": {
        "id": "raising-harrison"
      },
      "source": [
        "e. Using the output above, Please display the accuracy (number of correctly solved analogies), and then pick four categories of your choice, and display for each of them the accuracy, a correctly-solved analogy, and an incorrectly-solved one.  How many analogy tasks are there in total?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "collaborative-confidence",
      "metadata": {
        "id": "collaborative-confidence"
      },
      "outputs": [],
      "source": [
        "# Please write your Python code below and execute it.\n",
        "acc = analogy_scores[0]\n",
        "print(f'Accuracy: {acc}')\n",
        "# last element is total accuracy - so just count them together\n",
        "print(f\"Total of {(len(analogy_scores[1][-1]['correct'])+len(analogy_scores[1][-1]['incorrect']))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sorted-violin",
      "metadata": {
        "id": "sorted-violin"
      },
      "source": [
        "f. Create a short file called `questions-words-NAME.txt` (where `NAME` is your name) with several new test items for analogies (at least 10 lines), following the template of `questions-words.txt`.  For instance, from the three following pairs: (eye, see), (ear, listen), and (foot, walk) you can create 12 test items, varying the item that the system must predict and the initial items.  What is the accuracy of the model on your test set?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hundred-installation",
      "metadata": {
        "id": "hundred-installation"
      },
      "outputs": [],
      "source": [
        "# Please write your Python code below and execute it.\n",
        "analogy_scores_self = wv_model.evaluate_word_analogies('questions-words-florian-baer.txt')\n",
        "print(analogy_scores_self[0])\n",
        "analogy_scores_self = wv_model.evaluate_word_analogies('questions-words-florian-baer2.txt')\n",
        "print(analogy_scores_self[0])\n",
        "analogy_scores_self = wv_model.evaluate_word_analogies('questions-words-florian-baer3.txt')\n",
        "print(analogy_scores_self[0])\n",
        "# does not work too good"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "competent-usage",
      "metadata": {
        "id": "competent-usage"
      },
      "source": [
        "## 3. Training a word2vec model from scratch\n",
        "In this section, you will first use `gensim.downloader` to retrieve a 100-million character corpus ('text8' excerpt from Wikipedia).  You will use this data to train your own word2vec model.  Then, you will test the model on word similarity and analogies tasks.\n",
        "* [documentation of gensim.downloader](https://radimrehurek.com/gensim/downloader.html)\n",
        "* [corpora and pre-trained models available from gensim-data](https://github.com/RaRe-Technologies/gensim-data) -- the list can also be accessed with the command `gensim.downloader.info()` \n",
        "\n",
        "Please run the following code first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "satellite-interface",
      "metadata": {
        "id": "satellite-interface"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api\n",
        "text8_corpus = api.load('text8') # Downloads file once if needed -- if not, loads it from local copy."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "august-meaning",
      "metadata": {
        "id": "august-meaning"
      },
      "source": [
        "a. How many words are there in the 'text8' corpus?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "another-vatican",
      "metadata": {
        "id": "another-vatican"
      },
      "outputs": [],
      "source": [
        "# Please write your Python code below and execute it.\n",
        "words_set = set()\n",
        "words = []\n",
        "[words_set.add(word)for sentence in text8_corpus for word in sentence]\n",
        "[words.append(word)for sentence in text8_corpus for word in sentence]\n",
        "print(f'Unique words: \\t{len(words_set)}')\n",
        "print(f'All words: \\t\\t{len(words)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "steady-brown",
      "metadata": {
        "id": "steady-brown"
      },
      "source": [
        "b. Using the [documentation of Gensim's Word2Vec class](https://radimrehurek.com/gensim/models/word2vec.html), train your own word2vec model with 100-dimensional embeddings using 'text8'.  How many seconds does this take? (Use the difference between start and end times obtained with `time.time()`.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "blond-semiconductor",
      "metadata": {
        "id": "blond-semiconductor"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from gensim.models import Word2Vec\n",
        "# Please write your Python code below and execute it.\n",
        "start = time.time()\n",
        "model = Word2Vec(sentences=text8_corpus, vector_size=100, window=5, min_count=1, workers=8)\n",
        "end = time.time()\n",
        "duration = end - start\n",
        "print(f'Duration is {duration:.2f} seconds')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tribal-viking",
      "metadata": {
        "id": "tribal-viking"
      },
      "source": [
        "c. Using your code from Section 1, what are the vocabulary size and the dimensionality of the embedding space of this model?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "average-toolbox",
      "metadata": {
        "id": "average-toolbox"
      },
      "outputs": [],
      "source": [
        "# Please write the Python code needed to display the vocabulary size and execute it.\n",
        "\n",
        "print(f'vocabulary size: {len(model.wv.vectors)}')\n",
        "print(f'vector size: {model.wv.vector_size}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "complex-respondent",
      "metadata": {
        "id": "complex-respondent"
      },
      "source": [
        "d. Please read the \"*Usage examples*\" of the [Word2Vec class](https://radimrehurek.com/gensim/models/word2vec.html) to understand the difference between saving the full Word2Vec model (which enables future retraining on additional data) or saving only the vectors, an instance of KeyedVectors, which will save space.  Now, (1) save the vectors only, (2) load the vectors into a new variable, and (if everything worked fine), (3) delete the old model variable from the notebook's memory using `del`.  Note: saving the vectors may create one or more files, depending on the size of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "standard-development",
      "metadata": {
        "id": "standard-development"
      },
      "outputs": [],
      "source": [
        "# Please write your Python code below and execute it.\n",
        "word_vectors = model.wv\n",
        "word_vectors.save(\"mymodel.wordvectors\")\n",
        "\n",
        "del model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qpqhQUVm2UZO",
      "metadata": {
        "id": "qpqhQUVm2UZO",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "wv_text8 = KeyedVectors.load(\"mymodel.wordvectors\", mmap='r')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "backed-modern",
      "metadata": {
        "id": "backed-modern"
      },
      "source": [
        "e. Evaluate the new model on WordSimilarity-353 and Analogies tasks, reusing your code from above.  How does this model compare with the one trained on Google News?  Why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "domestic-evaluation",
      "metadata": {
        "id": "domestic-evaluation"
      },
      "outputs": [],
      "source": [
        "#wv_text8.init_sims(replace=True) # see (2b) but less important as the model is much smaller\n",
        "\n",
        "# Please write your Python code below and execute it.\n",
        "\n",
        "pearson2, spearman2, oov_ratio2 = wv_text8.evaluate_word_pairs(datapath('wordsim353.tsv'))\n",
        "print(f'Pearson {pearson2[0]:2f} with p-value {pearson2[1]}')\n",
        "\n",
        "analogy_scores2 = wv_text8.evaluate_word_analogies(datapath('questions-words.txt'))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wBN_WzcX2UZQ",
      "metadata": {
        "id": "wBN_WzcX2UZQ",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "acc2 = analogy_scores2[0]\n",
        "print(f'Accuracy: {acc2}')\n",
        "# last element is total accuracy - so just count them together\n",
        "print(f\"Total of {(len(analogy_scores2[1][-1]['correct'])+len(analogy_scores2[1][-1]['incorrect']))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dramatic-celtic",
      "metadata": {
        "id": "dramatic-celtic"
      },
      "outputs": [],
      "source": [
        "# Please write below a short comment to compare the 'Text8' model with the'Google News' model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DxbJggDg2UZR",
      "metadata": {
        "collapsed": false,
        "id": "DxbJggDg2UZR"
      },
      "source": [
        "The models pearson correlation is about 3 % worse than the original model trained from google news.\n",
        "Regarding the analogy task, the self trained model is about 10% worse which in my opinion can be explained by the significant reduction of the vectors representing the words in the vectorspace"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "original-incident",
      "metadata": {
        "id": "original-incident"
      },
      "source": [
        "f. Compare the accuracies on the analogy tasks of the two models for each category of tasks.  For which category are accuracies the most similar.  Can you explain this?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "comfortable-prospect",
      "metadata": {
        "id": "comfortable-prospect"
      },
      "outputs": [],
      "source": [
        "# Please write your Python code below and execute it.\n",
        "data = []\n",
        "for i, score in enumerate(analogy_scores[1]):\n",
        "    data.append((analogy_scores[1][i]['section'],\n",
        "     len(analogy_scores[1][i]['correct'])/(len(analogy_scores[1][i]['correct'])+len(analogy_scores[1][i]['incorrect'])),\n",
        "     len(analogy_scores2[1][i]['correct'])/(len(analogy_scores2[1][i]['correct'])+len(analogy_scores2[1][i]['incorrect']))))\n",
        "print(data)\n",
        "data.sort(key=lambda x: x[1]-x[2])\n",
        "print('Highest similarity:')\n",
        "print(data[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nwTH_0LV2UZS",
      "metadata": {
        "collapsed": false,
        "id": "nwTH_0LV2UZS"
      },
      "source": [
        "The highest similarity in the accuracy is in the `gram1-adjective-to-adver` - a wild guess is that the similarity is the biggest because the accuracy on the google news trained model is very low."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "numeric-sunday",
      "metadata": {
        "id": "numeric-sunday"
      },
      "source": [
        "## 4. Compare the two models on your own analogy tasks\n",
        "In this section, you will evaluate the new model on the analogy tasks you defined in Section 2f.  You will then try to diagnose the performance by inspecting the word vectors.\n",
        "\n",
        "a. Reusing the code from above, what is the accuracy of the model trained on Text 8 on your analogy tasks from 2f?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "functional-control",
      "metadata": {
        "id": "functional-control"
      },
      "outputs": [],
      "source": [
        "# Please write your Python code below and execute it.\n",
        "analogy_scores_self_model_text8 = wv_text8.evaluate_word_analogies('questions-words-florian-baer.txt')\n",
        "print(analogy_scores_self[0])\n",
        "analogy_scores_self_model_text8 = wv_text8.evaluate_word_analogies('questions-words-florian-baer2.txt')\n",
        "print(analogy_scores_self[0])\n",
        "analogy_scores_self_model_text8 = wv_text8.evaluate_word_analogies('questions-words-florian-baer3.txt')\n",
        "print(analogy_scores_self[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "adverse-oxford",
      "metadata": {
        "id": "adverse-oxford"
      },
      "source": [
        "b. We are now going to visualize the word vectors for the words in your analogy task.  Store the list of words in a variable and check which ones are in the vocabulary; create a new variable with them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "informative-learning",
      "metadata": {
        "id": "informative-learning"
      },
      "outputs": [],
      "source": [
        "# Please write your Python code below and execute it.\n",
        "# not all words available: words = ['Airplane', 'Fly', 'Car', 'Drive', 'Human', 'Run']\n",
        "words = ['switzerland', 'cheese', 'italy', 'pizza', 'usa', 'burger']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "activated-marine",
      "metadata": {
        "id": "activated-marine"
      },
      "source": [
        "c. The function below will help you plot a 2D representation of the word vectors using [PCA from scikit.learn](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html).  (It is also possible to use [UMAP](https://umap-learn.readthedocs.io/en/latest/basic_usage.html) instead of PCA in display_scatterplot).  Please display the word vectors for your model trained on Text8, and then for the model trained on Google News.  Please comment on the differences. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "demographic-rabbit",
      "metadata": {
        "id": "demographic-rabbit"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "starting-catholic",
      "metadata": {
        "id": "starting-catholic"
      },
      "outputs": [],
      "source": [
        "def display_scatterplot(model, words): # assumes all words are in the vocabulary\n",
        "    word_vectors = [model[word] for word in words]\n",
        "    twodim = PCA().fit_transform(word_vectors)[:,:2]\n",
        "    plt.figure(figsize=(6,6))\n",
        "    plt.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r')\n",
        "    for word, (x,y) in zip(words, twodim):\n",
        "        plt.text(x + 0.03, y + 0.03, word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "innovative-portal",
      "metadata": {
        "id": "innovative-portal"
      },
      "outputs": [],
      "source": [
        "# Please write your Python code below and execute it.\n",
        "display_scatterplot(wv_text8, words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "unlikely-jackson",
      "metadata": {
        "id": "unlikely-jackson"
      },
      "outputs": [],
      "source": [
        "# Please write your Python code below and execute it.\n",
        "display_scatterplot(wv_model, words)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pharmaceutical-lyric",
      "metadata": {
        "id": "pharmaceutical-lyric"
      },
      "source": [
        "## End of Lab 6\n",
        "Please make sure all cells have been executed, save this completed notebook, compress it to a *zip* file, and upload it to [Moodle](https://moodle.msengineering.ch/course/view.php?id=1869)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "name": "MSE_AnTeDe_Lab6_Egli.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
